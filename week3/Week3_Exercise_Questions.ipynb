{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Exercise Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Session Outline:-\n",
    "1. Data Loading\n",
    "    * textFile\n",
    "    * parallelize\n",
    "    * from Master in a distributed environment (Week 3)\n",
    "2. MapReduce\n",
    "    * transformation & actions\n",
    "        * map & collect\n",
    "        * filter & count\n",
    "        * flatMap & take\n",
    "        * filter & reduce\n",
    "        * union, intersection\n",
    "        * join, leftOuterJoin, cartesian, cogroup\n",
    "        * reduceByKey & collect\n",
    "3. Config\n",
    "    * using external files (Week 3)\n",
    "    * using API\n",
    "    * Spark Session (discussed with Spark SQL)\n",
    "4. Partitions\n",
    "    * fill it with random data\n",
    "    * find/use the partitions    \n",
    "5. Spark SQL\n",
    "    * Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to remember:- \n",
    "* Learn your hardware configuration like RAM, CPU cores, etc.\n",
    "* All Spark functions will be given along with the questions, you have to fill Spark function with their respective parameters <br /> and write the corresponding Scala or Python Logic\n",
    "* This is a practice session, so no scores are calculated \n",
    "* For quicker programming, we will use the shell environment today\n",
    "* If your IDE configurations aren't working, approach us after the exercise session\n",
    "* Ofcourse, Solutions will be provided for these questions after this exercise session\n",
    "* If you are already familiar with the contents listed above, go ahead in learning Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration:-\n",
    "\n",
    "https://github.com/shathil/BigDataExercises\n",
    "\n",
    "From the above url, download the git repository as a zip file. From the dowloaded zip package, extract a folder called resources and place it inside your spark 2.1 folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster\n",
    "http://spark.apache.org/docs/1.1.0/cluster-overview.html\n",
    "\n",
    "### Standalone\n",
    "http://spark.apache.org/docs/1.1.0/spark-standalone.html\n",
    "    \n",
    "### Hadoop YARN\n",
    "http://spark.apache.org/docs/1.1.0/running-on-yarn.html\n",
    "\n",
    "### Apache Mesos\n",
    "http://spark.apache.org/docs/1.1.0/running-on-mesos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
