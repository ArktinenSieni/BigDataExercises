{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3 Exercise Questions\n",
    "## Today's Session Outline:- \n",
    "1. Lecture 3 Examples\n",
    "    * Review\n",
    "    * Shuffling\n",
    "    * Partitions\n",
    "    * Closures\n",
    "    * Cache and Persist\n",
    "    * Broadcast Variables\n",
    "    * Accumulators\n",
    "2. Spark SQL\n",
    "    - Review\n",
    "    - Data Sources \n",
    "3. Exercise 1 discussion\n",
    "\n",
    "## Next Week session Outline:-\n",
    "* TBA\n",
    "* Cluster environment configuration\n",
    "* SparkConfig using external files\n",
    "* Exercise 1 Solutions\n",
    "* Exercise 2 discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Things to remember:- TODO\n",
    "* Learn your hardware configuration like RAM, CPU cores, etc.\n",
    "* All Spark functions will be given along with the questions, you have to fill Spark function with their respective parameters <br /> and write the corresponding Scala or Python Logic\n",
    "* This is a practice session, so no scores are calculated \n",
    "* For quicker programming, we will use the shell environment today\n",
    "* If your IDE configurations aren't working, approach us after the exercise session\n",
    "* Ofcourse, Solutions will be provided for these questions after this exercise session\n",
    "* If you are already familiar with the contents listed above, go ahead in learning Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Configuration:-\n",
    "\n",
    "https://github.com/mohanprasath/BigDataExercises/tree/master/week3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Lecture 3 Examples\n",
    "\n",
    "### Review\n",
    "* RDD transofrmations are lazy.\n",
    "* Persist is storing data in different levels.\n",
    "* Cache(actually a specialized Persist) stores it only in memory. \n",
    "* Narrow transformations - map, filter\n",
    "    When output RDD is created from a single RDD\n",
    "* Wide transformations - groupByKey, reduceByKey\n",
    "    When output RDD is created from multiple RDDs, causing Shuffle\n",
    "* Partition - Refer Last Week's Exercises session notes for examples\n",
    "* Closure - \n",
    "* Shared variables - \n",
    "    * Broadcast variables - \n",
    "    * Accumulators - \n",
    "\n",
    "\n",
    "### Sources: \n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-caching.html\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-transformations.html\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-shuffle.html\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-dagscheduler.html\n",
    "\n",
    "\n",
    "## 1.1 Shuffle\n",
    "\n",
    "* By default, Spark manages the data handling in the behind using Map & Reduce tasks. \n",
    "* But during shuffle, we lose precious disk and network resources. \n",
    "\n",
    "### RDD Shuffle\n",
    "    Have you noticed your console during heavy data processing?. You might have seen something like this, <br />\n",
    "    [Stage 2:=======>                                             (143 + 20) / 1000]<br />\n",
    "    So what's a stage?. Wide dependncies are staged, while some narrow dependencies are grouped into a stage.\n",
    "    You can find the stages for your tasks executed in a Spark session at, <br\\>\n",
    "    http://localhost:4040/stages/\n",
    "    For example,\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 1 - Filtering random numbers\n",
    "* create an RDD with a group of random numbers \n",
    "* remove the negative values\n",
    "* Now filter it using a range condition like using > and <\n",
    "* did you notice any cnages in Web UI before performing actions\n",
    "* count each filtered RDD\n",
    "* are there any shuffling noticed in Web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 2 - Word count\n",
    "* create a function for Word count on README.md in your Spark home folder\n",
    "* use a reduceByKey function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 3\n",
    "* In above two examples, if you notice the stages in your Spark Web UI, they are different in execution.\n",
    "* The function numbers_count finishes in a single stage.\n",
    "* While the function word_count uses two stages.\n",
    "* Can you associate the types of depoendencies here?.\n",
    "* During stages, some actions like 'reduceByKey' requires data to be moved across partition. Refer Lecture 3 Partition slide Page 10. \n",
    "* The stages follow Topological order. Check the DAG overview in Spark Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 4 - Compare groupByKey and reduceBykey\n",
    "* Extract the zip file and perform word count using groupByKey and reduceBykey\n",
    "    http://www.ibiblio.org/webster/gcide_xml-0.51.zip\n",
    "* Analyse the difference in execution in the Web UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 Partition\n",
    "* __Why data is partitioned? __\n",
    "    - Allowing workers to have efficient data on which they execute\n",
    "    - We can define the number of partitions to exists in parallelize or textFile methods\n",
    "    - no of Partitions is limited to the total number of cores on all executor nodes\n",
    "* __Used RDD type : pairRDD__\n",
    "* __Types of Paritioning__\n",
    "    - Hash partition - not balanced, due to chaining\n",
    "    - Ranged parition - balanced, when keys are in different range <br/>\n",
    "        Hint :- Picking a key from the data is difficult. But usually all database table have primary keys, you can use them during range partitioning.\n",
    "* __Explicit Partitioning__\n",
    "    - __partitionBy__ \n",
    "        * used on an RDD\n",
    "        * takes an partitioner object as an argument\n",
    "        * NEEDS TO BE PERSISTED IN THE END\n",
    "    - __using transformation__ \n",
    "        * we create a partionable RDD using transformations sometimes. For example, pairRDD\n",
    "        * Syntax of a pairRDD : RDD[(K, V)]\n",
    "        * Some default settings are used by Spark to use either RangePartitioner(?) or HashPartitioner when required\n",
    "        * Refer Slide no 31 in shuffle_partition.pdf for more detail\n",
    "        * The partitioner used with the transformation is removed at the result\n",
    "* __Examples__\n",
    "    * Refer the DAG in Spark Web UI for word Count to learn the internal of transformation\n",
    "    * partitionBy is shown below\n",
    "    * RangePartitioner is yet to be implemented in Python (NEEDS VERIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 5\n",
    "* Use partitionBy on Question 4\n",
    "* Analyse the difference when using partitions - Explained in Last Exercise Session in Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Additional materials\n",
    "Hash Partitioner :- http://stackoverflow.com/questions/31424396/how-does-hashpartitioner-work\n",
    "\n",
    "Hash Tables :- (Switch to mute during the intro for like 10 seconds) https://www.youtube.com/watch?v=h2d9b_nEzoA\n",
    "\n",
    "Wikipedia :- https://en.wikipedia.org/wiki/Partition_(database)\n",
    "\n",
    "Custom Partition Example :- http://stackoverflow.com/questions/23127329/how-to-define-custom-partitioner-for-spark-rdds-of-equally-sized-partition-where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3 Closures \n",
    "* Anything that survives after it's required duration, or becomes closed in it's access\n",
    "* They some time cause errors during Serialization\n",
    "* Examples with next Tuesday session in a cluster setup\n",
    "\n",
    "### References :-\n",
    "\n",
    "http://stackoverflow.com/questions/36636/what-is-a-closure\n",
    "\n",
    "https://en.wikipedia.org/wiki/Closure_(computer_programming)\n",
    "\n",
    "A simple C++ example for Closure,\n",
    "\n",
    "http://mikehadlow.blogspot.fi/2011/07/what-is-closure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.4 Cache & Persist \n",
    "* __cache__ is a specialized version of __persist__\n",
    "* cache store the values only to the memory\n",
    "* If an RDD size exceeds memory, they will be computed on the fly\n",
    "* __persist__ can store an RDD to disk, memory, or both\n",
    "* Detailed settings can be found here\n",
    "\n",
    "http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence\n",
    "\n",
    "### Examples:-\n",
    "* try out with larger datasets, so storage preferences will become clear. \n",
    "* In the following example, we use generated data. So the data is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Question 6\n",
    "* create a random set of numbers\n",
    "* store it using cache and persist \n",
    "* Use Web UI for further analysis\n",
    "\n",
    "### Hint: \n",
    "* unpersist before either cache or persist\n",
    "* If you face memory error, try to increase driver memory.\n",
    "* Set the following when initializing the SparkContext\n",
    "* park.driver.memory 4000m\n",
    "\n",
    "#### Refer here:-\n",
    "http://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.5 Broadcast Variables\n",
    "* Spark stores Read-Only variables in each Machine, thus reducing the need for sending the variable with the tasks\n",
    "* Data is stored as serialized and de-serialized when it needs to be read.\n",
    "* Never change the variable after being broadcasted\n",
    "* Maintaining consistency falls upon the user\n",
    "\n",
    "# Question 7\n",
    "* declare a broadcast variable and access it using value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.6 Accumulators\n",
    "\n",
    "* Variable that depend upon multiple operations, for example: a __counter__ variable\n",
    "* Accumulators are stored in parallel to other short-lived variables.\n",
    "\n",
    "# Question 8\n",
    "* declare an accumulator variable and access it using the value function\n",
    "\n",
    "## Additional Material and Sources for Section 1:- \n",
    "\n",
    "* Some Known errors\n",
    "\n",
    "    http://stackoverflow.com/questions/29717257/pyspark-groupbykey-returning-pyspark-resultiterable-resultiterable?rq=1 \n",
    "\n",
    "\n",
    "* Remember DAG in Spark?. Refer Page 7 in, \n",
    "\n",
    "    https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf \n",
    "\n",
    "\n",
    "* If you submit your tasks to a DAG schedules, you can check stages visually in Spark Web UI. \n",
    "\n",
    "* interested in creating a custom scheduler, you can start from here,\n",
    "    \n",
    "    http://stackoverflow.com/questions/39471601/how-to-create-a-custom-apache-spark-scheduler\n",
    "    \n",
    "    http://spark.apache.org/docs/latest/job-scheduling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Apache SQL\n",
    "\n",
    "## Review\n",
    "* SparkSession\n",
    "* DataFrames\n",
    "* GlobalTemporaryView\n",
    "* Using SQL Queries\n",
    "\n",
    "## 2.1 DataFrames\n",
    "* new component from Apache Spark 1.6 and above\n",
    "* acts as a Distributed SQL Query Engine\n",
    "* Organized based upon Column names\n",
    "\n",
    "## 2.2 DataSets\n",
    "* Traditional data storage model\n",
    "* Handled input types\n",
    "* Python implementation is Yet to be added (No Official Documentation Exists)\n",
    "* For creatinf DataSets in Scala or Java, refer\n",
    "    http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets\n",
    "\n",
    "## 2.3 Handled Input Types\n",
    "* json\n",
    "    spark_session.read.json\n",
    "* text\n",
    "    spark_session.read.text\n",
    "* Apache Parquet files\n",
    "* Apache Avro files\n",
    "* Hadoop files\n",
    "* Cassandra database.\n",
    "\n",
    "# Question 9 - Review\n",
    "* Create a Spark Session\n",
    "* read the people.json from resources folder and store it into a dataframe\n",
    "* use show and printSchema function in the dataframe created from people.json\n",
    "* use select and groupBy on the dataframe\n",
    "\n",
    "\n",
    "# Question 10 - Creating a Data Frame from an RDD\n",
    "* Code DEMO\n",
    "\n",
    "# 3 Exercise 2 Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
