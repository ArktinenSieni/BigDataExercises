{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Apache Spark Streaming\n",
    "\n",
    "http://spark.apache.org/streaming/\n",
    "\n",
    "__Documentation URL:__\n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "\n",
    "__Python reference:__\n",
    "http://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "__Scala reference:__\n",
    "http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1\n",
    "__ You need this :__ a __StreamingContext__ object to do any streaming task, similar to a SparkContext.\n",
    "    \n",
    "__ Scala Example :__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "\n",
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Example\")\n",
    "val ssc = new StreamingContext(conf, Seconds(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__ Python Example: __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"Example\")  # Created a SparkContext object and is being passed to the StreamingContext\n",
    "ssc = StreamingContext(sc, batchDuration=1)  # batchDuration accepts value in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2\n",
    "\n",
    "__ You need this :__ a __DStream__ object, its a sequence of RDDs.\n",
    "    \n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams\n",
    "\n",
    "__Input Sources:__ The following examples, use a TCP Socket as an input sources. We can group the input types as,\n",
    "\n",
    "1. Basic sources : Sockets, File systems\n",
    "    http://spark.apache.org/docs/latest/streaming-programming-guide.html#basic-sources\n",
    "    \n",
    "2. Advanced sources : Kafka, Flume, etc\n",
    "    http://spark.apache.org/docs/latest/streaming-programming-guide.html#advanced-sources\n",
    "\n",
    "__ Scala Example :__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "val lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Python Example:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__\n",
    "\n",
    "Receiver(Scala and Java documentation) is available. Receivers are responsible for handling the data from the input source and store it for spark streaming access. For the example, we are using a single data source. When you use multiple data sources, the cores allocated to the workers n, should be greater than the number of input sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "__Start the stream:__ Like \"actions\", the real computation start after the start command being issued. Only one StreamingContext can be allowed in a Spark Session. But you can use multiple input streams, and don't forget to allocate enough workers for processing those input streams. \n",
    "\n",
    "ssc.start()\n",
    "\n",
    "__Note:__ Once the streaming context is started no new code can be added. Stopping the context is similar to Spark Context, you can use\n",
    "\n",
    "ssc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "All action performed on the DStreams are done in parallel, so to collect the final result, or update it periodically, use the following,\n",
    "\n",
    "__UpdateStateByKey__\n",
    "\n",
    "Transofrmations on RDD-to-RDD are still allowed in DStream. \n",
    "\n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations\n",
    "\n",
    "__Windowed Operations__ are performed over a DStream containing a discrete data. Here a window represents a collection of such discrete data. DStream considered data in a discrete fashion and uses a single DStream till the end. While using window operation, the data is grouped into multiple windows. Windows may overlap to each other but the final results produce a discrete window too. \n",
    "\n",
    "__Two properties to specify :-__\n",
    "1. window length\n",
    "2. sliding interval\n",
    "\n",
    "The above properties are required as argument for any window operations.\n",
    "\n",
    "A Databricks example,\n",
    "\n",
    "https://docs.cloud.databricks.com/docs/latest/databricks_guide/07%20Spark%20Streaming/10%20Window%20Aggregations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "__ Steps to deploy a spark streaming application__\n",
    "\n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications\n",
    "    \n",
    "__ Window and Sliding window __\n",
    "\n",
    "https://groups.google.com/forum/#!topic/spark-users/GQoxJHAAtX4\n",
    "\n",
    "http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
