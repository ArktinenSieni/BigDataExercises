{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple Setup - Single Master and a Worker\n",
    "\n",
    "https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-standalone-example-2-workers-on-1-node-cluster.adoc\n",
    "\n",
    "__1. Go to spark home folder in terminal, then navigate to sbin folder. Now start your master using the following command, __\n",
    "\n",
    "./start-master.sh\n",
    "\n",
    "__2. Then in your browser go to the following URL,__\n",
    "\n",
    "localhost:8080\n",
    "\n",
    "__3. If Spark master has successfully started, then you would see a web page with Spark info. After the Spark logo, you will see a line begining with thw word URL: For example,__\n",
    "\n",
    "URL: spark://m:7077\n",
    "\n",
    "\n",
    "__4. The above URL is the master URL. You can add it to your program's spark configuration. __\n",
    "\n",
    "For example, In Python,<br/>\n",
    "<br/>\n",
    "from pyspark import SparkConf, SparkContext<br/>\n",
    "conf = (SparkConf().setMaster(\"spark://m:7077\").setAppName(\"Examples\"))<br/>\n",
    "sc = SparkContext(conf=conf)<br/>\n",
    "\n",
    "For Example, In Scala,<br/>\n",
    "<br/>\n",
    "val conf = new SparkConf().setAppName(\"Examples\").setMaster(\"spark://m:7077\")<br/>\n",
    "val sc = new SparkContext(conf)<br/>\n",
    "\n",
    "__5. Now the next step is to start the worker, In your terminal in SPARK_HOME/sbin/ folder, type the following command, __\n",
    "\n",
    "./start-slave.sh spark://m:7077<br/>\n",
    "\n",
    "__Here the command \"./start-slave.sh\" is followed by a space \" \", then the Master's URL is passed as an argument. You can check your localhots:8080, for your worker, after executing the above command. __\n",
    "\n",
    "__6. If you execute any command, the master and Slave will automatically, allocate memory for the tasks and use your cores efficiently. Test it with any example. __\n",
    "\n",
    "\n",
    "## Official Documentation\n",
    "http://spark.apache.org/docs/latest/spark-standalone.html\n",
    "\n",
    "\n",
    "Note: Set the following environment variables,<br/>\n",
    "\n",
    "For Python3, (Only in the case if you haven't set it up earlier),<br/>\n",
    "export PYSPARK_PYTHON=\"/usr/bin/python2\"<br/>\n",
    "export PYSPARK_DRIVER_PYTHON=\"python2\"<br/>\n",
    "\n",
    "For Scala,<br/>\n",
    "None newly required. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multiple Worker Setup - Single Master and Multiple Worker using start-slave.sh  \n",
    "\n",
    "### NOTE - YOU HAVE TO SHUT DOWN (n-1) WORKERS MANUALLY\n",
    "\n",
    "https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-standalone-example-2-workers-on-1-node-cluster.adoc\n",
    "\n",
    "__1. Follow the instructions in above section for starting the master __\n",
    "__2. While starting the slaves, you can user create a configuration file conf/spark-env.sh as shown in the above documentation or you can ude the folllowing command below, __\n",
    "\n",
    "SPARK_WORKER_INSTANCES=4 ./sbin/start-slave.sh spark://m:7077\n",
    "\n",
    "__3. If you look closely, I haven't specified anything else except the number of workers needed. All other options are allocated greedily by the Spark.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multiple Worker Setup - Single Master and Multiple Worker using start-slaves.sh  \n",
    "\n",
    "https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-standalone-example-2-workers-on-1-node-cluster.adoc\n",
    "\n",
    "https://www.cs.helsinki.fi/ukko/hpc-report.txt\n",
    "\n",
    "__1. Follow the instructions in above section for starting the master __\n",
    "\n",
    "__2. Now create a ssh key and add it to the ssh-agent__<br/>\n",
    "     cd ~/.ssh/<br/>\n",
    "     ssh-copy-id localhost<br/>\n",
    "     eval '$(ssh-agent -s)'<br/>\n",
    "     ssh-add ~/.ssh/id_rsa<br/>\n",
    "     After all these, try issuing the command, <br/>\n",
    "        ./sbin/start-slaves.sh spark://m:7077<br/>\n",
    "\n",
    "Note: \n",
    "You can also set up password less slaves,to reduce time taken during setup. There are several great tutorials available.\n",
    "\n",
    "## Hint:\n",
    "1. try using this command before ssh-copy-id command <br/> \n",
    "    ssh localhost<br/>\n",
    "2. For Mac, Try allowing remote sharing in your preferences, then start use ssh localhost\n",
    " command.\n",
    " \n",
    "### References:\n",
    "https://mbonaci.github.io/mbo-spark/<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cluster in UKKO\n",
    "\n",
    "https://www.cs.helsinki.fi/ukko/hpc-report.txt\n",
    "\n",
    "### Refer the following Linux Fundamentals course on Setting up the SSH keys\n",
    "https://wiki.helsinki.fi/display/linuxfun2013/Week1\n",
    "    \n",
    "__1. Login in to melkki. Then into an ukko node where you want to run your master. There create an ssh key in any one of the UKKO nodes where you want to start your master<br/>__\n",
    "__2. Now copy the ssh public key to all other ukko nodes where you want to create the workers<br/>__\n",
    "    \n",
    "    cat ~/.ssh/id_rsa.pub | ssh ukkoxy1.hpc.cs.helsinki.fi \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"\n",
    "    cat ~/.ssh/id_rsa.pub | ssh ukkoxy2.hpc.cs.helsinki.fi \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"\n",
    "    cat ~/.ssh/id_rsa.pub | ssh ukkoxy3.hpc.cs.helsinki.fi \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"\n",
    "    cat ~/.ssh/id_rsa.pub | ssh ukkoxy4.hpc.cs.helsinki.fi \"mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys\"\n",
    "       \n",
    "In the above command I have chosen ukkoxy1, ukkoxy2, ukkoxy3, and ukkoxy4 as my worker nodes for my master node in ukkoxy0. \n",
    "Remember you now have unrestricted ssh access between ukkoxy0 and the other worker nodes. <br/>\n",
    "\n",
    "__3. Next step is to add the lsit of workers host name into conf/slaves file.<br/>__\n",
    "    ukkoxy1.hpc.cs.helsinki.fi<br/>\n",
    "    ukkoxy2.hpc.cs.helsinki.fi<br/>\n",
    "    ukkoxy3.hpc.cs.helsinki.fi<br/>\n",
    "    ukkoxy4.hpc.cs.helsinki.fi<br/>\n",
    "    \n",
    "__4. Now start your master using the following command,<br/>__\n",
    "    ./sbin/start-master.sh \n",
    "\n",
    "__5. To check if your master is running, use the following command,<br/>__\n",
    "    ps aux | grep spark\n",
    "    \n",
    "    It will list all running spark processes. For example,\n",
    "    \n",
    "    chinnasa 17765  2.2  0.9 5828172 298884 pts/0  Sl   12:00   0:04 /usr/lib/jvm/java-7-openjdk-amd64/bin/java -cp /cs/home/chinnasa/spark/spark-2.0.2-bin-hadoop2.7/conf/:/cs/home/chinnasa/spark/spark-2.0.2-bin-hadoop2.7/jars/* -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.master.Master --host ukko043.hpc.cs.helsinki.fi --port 7077 --webui-port 8080\n",
    "    \n",
    "    From the above command, you can figure out the port num where the process is running.<br/>\n",
    "\n",
    "__6. Final step is to run the spark slaves, use the following command,<br/>__\n",
    "    ./sbin/start-slaves.sh spark://ukkoxy0.hpc.cs.helsinki.fi:7077\n",
    "    <br/>\n",
    "    It should list where the .out files are stored.<br/>\n",
    "    \n",
    "__7. You can got to corresponsing ukko nodes and check the list of runnign processes usign the follwoing command, <br/>__\n",
    "    htop\n",
    "    use F10 to exit 'htop'. <br/>\n",
    "\n",
    "__8. To stop all, use the following command, __\n",
    "    ./sbin/stop-all.sh\n",
    "\n",
    "Congrats, now you have set up a Spark Standalone cluster with four slaves and a master. While runnign yout programs, use the following spark configuration settings,<br/>\n",
    "\n",
    "For Python,<br/>\n",
    "conf = (SparkConf().setMaster(\"spark://ukko043.hpc.cs.helsinki.fi:7077\").setAppName(\"Examples\"))<br/>\n",
    "sc = SparkContext(conf=conf)<br/>\n",
    "\n",
    "For Scala,<br/>\n",
    "val conf = new SparkConf().setAppName(\"week2\").setMaster(\"spark://ukko043.hpc.cs.helsinki.fi:7077\")<br/>\n",
    "val sc = new SparkContext(conf)<br/>\n",
    "\n",
    "NOTE: Note always your ssh public keys remain in the key ring, so when ever you need to run your workers, check if you have access without requiring password.\n",
    "\n",
    "Also there is some issue with python3 in UKKO, but python2 is working. For now, use python2 until the issue is resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
